# TODO: Clustering with Outlier Detection ðŸš§

::::{important}
ðŸš§ðŸš§ This chapter is under construction.  Please come back later. ðŸš§ðŸš§
::::


```{python imports,results="hide"}
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import genieclust
```

```{python settings,results="hide",echo=FALSE}
from tabulate import tabulate
np.set_printoptions(precision=3, threshold=50, edgeitems=50)
pd.set_option("display.min_rows", 200)
```



Let's load an example dataset that can be found
at the [**hdbscan**](https://hdbscan.readthedocs.io)
{cite}`hdbscanpkg` package's project site:

```{python noise-load}
dataset = "hdbscan"
X = np.loadtxt("%s.data.gz" % dataset, ndmin=2)
labels_true = np.loadtxt("%s.labels0.gz" % dataset, dtype=np.intp) - 1
n_clusters = len(np.unique(labels_true[labels_true>=0]))
```



Here are the reference labels as identified by a manual annotator.
The light grey markers corresponding to the label `-1` designate points
that can be considered outliers.

```{python noise-scatter,results="hide",fig.cap="Reference labels"}
genieclust.plots.plot_scatter(X, labels=labels_true, alpha=0.5)
plt.title("(n=%d, true n_clusters=%d)" % (X.shape[0], n_clusters))
plt.axis("equal")
plt.show()
```



## Smoothing Factor

The **genieclust** package allows for clustering with respect
to the mutual reachability distance, $d_M$,
known from the DBSCAN\* algorithm {cite}`hdbscan`.
This metric is parameterised by *a smoothing factor*, $M\ge 1$, which
controls how eagerly points are filtered out from the clustering process.

Namely, instead of the ordinary (usually Euclidean) distance
between two points $i$ and $j$, $d(i,j)$, we take
$d_M(i,j)=\max\{ c_M(i), c_M(j), d(i, j) \}$, where the so-called $M$-*core*
distance $c_M(i)$ is the distance between $i$ and its $M$-th nearest neighbour
(here, not including $i$ itself, unlike in the original paper).

DBSCAN\* and its hierarchical version, HDBSCAN\*, introduces the notion
of noise and core points.  Furthermore, their predecessor, DBSCAN,
also marks certain non-core values as border points.  They all rely
on a specific threshold $\varepsilon$ that is applied onto the points'
core distances.

In **genieclust** we identify anomalies slightly differently.
<!--
Following  {cite}`geniem`
TODO
-->
(ðŸš§ðŸš§ TODO: describe ðŸš§ðŸš§)


Here are the effects of playing with the $M$ parameter
(we keep the default `gini_threshold`):

```{python noise-Genie1,results="hide",fig.cap="Labels predicted by Genie with outlier detection",fig.height=5.9375}
Ms = [5, 10, 25, 50]
for i in range(len(Ms)):
    g = genieclust.Genie(n_clusters=n_clusters, M=Ms[i])
    labels_genie = g.fit_predict(X)
    plt.subplot(2, 2, i+1)
    genieclust.plots.plot_scatter(X, labels=labels_genie, alpha=0.5)
    plt.title("(gini_threshold=%g, M=%d)"%(g.gini_threshold, g.M))
    plt.axis("equal")
plt.show()
```

<!--
We can also detect outliers with Genie, remove them temporarily
from the dataset, and then apply the clustering procedure once again
but now with respect to the original distance (here: Euclidean):

```{python noise-Genie2,results="hide",fig.cap="Labels predicted by Genie when outliers were first removed from the dataset"}
# Step 1: Noise point identification
g1 = genieclust.Genie(n_clusters=n_clusters, M=50)
labels_genie = g1.fit_predict(X)
is_outlier = (labels_genie < 0)
# Step 2: Clustering of non-noise points:
g2 = genieclust.Genie(n_clusters=n_clusters)
labels_genie_no_outliers = g2.fit_predict(X[~is_outlier, :])
# Replace old labels with the new ones:
labels_genie[~is_outlier] = labels_genie_no_outliers
# Scatter plot:
genieclust.plots.plot_scatter(X, labels=labels_genie, alpha=0.5)
plt.title("(gini_threshold=%g, noise points removed first; M=%d)"%(g2.gini_threshold, g1.M))
plt.axis("equal")
plt.show()
```
-->

Contrary to the HDBSCAN\* method featured in the [**hdbscan**](https://hdbscan.readthedocs.io)
package {cite}`hdbscanpkg`, in our case, we can request a *specific* number of clusters.
Moreover, we can easily switch between partitions of finer or coarser
granularity.  As *Genie* is a hierarchical algorithm, the partitions are properly nested.


```{python noise-Genie3,results="hide",fig.cap="Labels predicted by Genie when outliers were removed from the dataset â€“ different number of clusters requested",fig.height=5.9375}
ncs = [4, 5, 6, 7]
for i in range(len(ncs)):
    g = genieclust.Genie(n_clusters=ncs[i], M=50)
    labels_genie = g.fit_predict(X)
    plt.subplot(2, 2, i+1)
    genieclust.plots.plot_scatter(X, labels=labels_genie, alpha=0.5)
    plt.title("(n_clusters=%d)"%(g.n_clusters))
    plt.axis("equal")
plt.show()
```



## A Comparision with HDBSCAN\*


Here are the results returned by [**hdbscan**](https://hdbscan.readthedocs.io)
with default parameters:

```{python noise-import-HDBSCAN}
import hdbscan
```

```{python noise-HDBSCAN1,results="hide",fig.cap="Labels predicted by HDBSCAN\\*."}
h = hdbscan.HDBSCAN()
labels_hdbscan = h.fit_predict(X)
genieclust.plots.plot_scatter(X, labels=labels_hdbscan, alpha=0.5)
plt.title("(min_cluster_size=%d, min_samples=%d)" % (
    h.min_cluster_size, h.min_samples or h.min_cluster_size))
plt.axis("equal")
plt.show()
```


By tuning up `min_cluster_size` and/or `min_samples` (which corresponds
to our `M`; by the way, `min_samples` defaults to `min_cluster_size`
if not provided explicitly), we can obtain a partition that is even closer
to the reference one:


```{python noise-HDBSCAN2,results="hide",fig.cap="Labels predicted by HDBSCAN\\* â€“ different settings",fig.height=5.9375}
mcss = [5, 10, 25]
mss = [5, 10]
for i in range(len(mcss)):
    for j in range(len(mss)):
        h = hdbscan.HDBSCAN(min_cluster_size=mcss[i], min_samples=mss[j])
        labels_hdbscan = h.fit_predict(X)
        plt.subplot(3, 2, i*len(mss)+j+1)
        genieclust.plots.plot_scatter(X, labels=labels_hdbscan, alpha=0.5)
        plt.title("(min_cluster_size=%d, min_samples=%d)" % (
            h.min_cluster_size, h.min_samples or h.min_cluster_size))
        plt.axis("equal")
plt.show()
```
