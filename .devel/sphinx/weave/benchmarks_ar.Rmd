# Benchmarks (How Good Is It?)

In this section we evaluate the usefulness of different clustering algorithms.
We use our [framework for benchmarking clustering algorithms (benchmark suite version 1.0.1)](https://github.com/gagolews/clustering-benchmarks)
{cite}`clustering-benchmarks` which aggregates datasets from various sources,
including, but not limited to {cite}`uci,kmsix,fcps,graves,chameleon,xnn`.
Ground-truth/reference label vectors are provided alongside each dataset.
They define the desired number of clusters. Hence, we only study
the algorithms that allow for setting of `n_clusters` explicitly.

We will apply a few agglomerative hierarchical
methods (average, centroid, complete, single, and Ward linkage; implemented in the
[fastcluster](http://www.danifold.net/fastcluster.html) package {cite}`fastclusterpkg`),
k-means, expectation-maximisation (EM) for Gaussian mixtures, Birch, spectral
(implemented in [scikit-learn](https://scikit-learn.org/) {cite}`sklearn`),
[ITM](https://github.com/amueller/information-theoretic-mst) {cite}`itm`,
and Genie {cite}`genieins`.

The adjusted Rand index (see {cite}`comparing_partitions`) will be used
to quantify the agreement between
a reference and a predicted clustering on the scale $[0,1]$,
with score of 1.0 denoting perfect agreement. However, as there might be
multiple equally valid/plausible/useful partitions (see also
{cite}`sdmc` and {cite}`clustering-benchmarks` for discussion),
the outputs generated by a single algorithm is evaluated
against all the available reference labellings and the maximal similarity score
is reported.

For more detailed results based on other partition similarity scores,
see the [Appendix](benchmarks_details).


```{python imports,results="hide",echo=FALSE}
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import os.path, glob, re
import scipy.stats
#from natsort import natsorted
#import genieclust
import sklearn.metrics
import seaborn as sns
import pweave
from tabulate import tabulate
np.set_printoptions(precision=3, threshold=50, edgeitems=50)
pd.set_option("display.min_rows", 200)
plt.style.use("bmh")
plt.rcParams.update({
    'font.size': 9,
    'font.family': 'sans-serif',
    'font.sans-serif': ['Ubuntu Condensed', 'Alegreya', 'Alegreya Sans']})


#``````````````````````````````````````````````````````````````````````````````
#``````````````````````````````````````````````````````````````````````````````
#``````````````````````````````````````````````````````````````````````````````

similarity_measure = "ar"


# Load results file:
res  = pd.read_csv("v1-scores.csv.gz")
#dims = pd.read_csv("v1-dims.csv")

# ari, afm can be negative --> replace negative indexes with 0.0
res.iloc[:,7:] = res.iloc[:,7:].transform(lambda x: np.maximum(x, 0.0), axis=1)

res = res.loc[res.method.isin([
    "Genie_G0.1", "Genie_G0.3", "Genie_G0.5", "Genie_G1.0", "ITM",
    "fastcluster_complete", "fastcluster_centroid", "fastcluster_average",
    "fastcluster_ward", "sklearn_kmeans", "sklearn_gm", "sklearn_spectral_Arbf_G5",
    "sklearn_birch_T0.01_BF100"]), :]

res["method"] = res["method"].map({
    "Genie_G0.1": "Genie_0.1",
    "Genie_G0.3": "Genie_0.3",
    "Genie_G1.0": "Single linkage",
    "ITM": "ITM",
    "Genie_G0.5": "Genie_0.5",
    "fastcluster_complete": "Complete linkage",
    "fastcluster_average": "Average linkage",
    "fastcluster_centroid": "Centroid linkage",
    "fastcluster_ward": "Ward linkage",
    "sklearn_kmeans": "K-means",
    "sklearn_gm": "Gaussian mixtures ",
    "sklearn_spectral_Arbf_G5": "Spectral_RBF_5",
    "sklearn_birch_T0.01_BF100": "Birch_0.01",
    })


#``````````````````````````````````````````````````````````````````````````````
#``````````````````````````````````````````````````````````````````````````````
#``````````````````````````````````````````````````````````````````````````````
# Let's first inspect the missing clusterings.

# TODO: identify missing cases, assign them with scores == 0.0


# how_many_labels_true_considered = \
#     res.groupby(["dataset", "method"]).ar.count().\
#         unstack(fill_value=0).stack().rename("how_many_labels").reset_index()
# num_labels_true = res.loc[:,["dataset", "labels"]].drop_duplicates().\
#     groupby(["dataset"]).labels.count()
#
#
# # Number of missing comparisons:
# (
#     num_labels_true-
#     how_many_labels_true_considered.
#         set_index(["dataset", "method"]).iloc[:,0]
# ).rename("num_missing").reset_index().groupby("method").num_missing.\
#     sum().sort_values().reset_index().query("num_missing>0")
#
# # what's missing
# _method = "sklearn_spectral_Arbf_G5"
# _all = pd.Series(res.dataset.unique())
# _all[~_all.isin(res.loc[res.method==_method, "dataset"])]


#``````````````````````````````````````````````````````````````````````````````
#``````````````````````````````````````````````````````````````````````````````
#``````````````````````````````````````````````````````````````````````````````


def show_ranks(res_max):
    r = lambda x: scipy.stats.rankdata(-x, method="min")
    _dat = res_max.set_index(["dataset", "method"])[similarity_measure].unstack().\
        round(2).T.apply(r).T.describe().T.round(1)
    print(tabulate(_dat, _dat.columns, tablefmt="github"), "\n\n")


def do_plot(res_max):
    # which similarity measure to report below:
    __order=res_max.groupby("method")[similarity_measure].mean().sort_values(ascending=False).index
    plt.rcParams["figure.figsize"] = (9,7)
    sns.boxplot(y="method", x=similarity_measure, data=res_max,
                order=__order,
                orient="h",
                showmeans=True,
                meanprops=dict(markeredgecolor="k", marker="x"))
    plt.show()
```


## Small Datasets

As some of the algorithms tested here have failed to generate a solution
within reasonable time limits (e.g., spectral clustering),
in this part we restrict ourselves to the datasets with up to 10,000 observations.
As suggested in the benchmark suite's description, we omit the over-populous
"parametric" Gaussian-distributed batteries `h2mg` and `g2mg`.

Here are the boxplots of the empirical distributions of the adjusted Rand index.
We report the results for Birch and spectral clustering with parameters
that lead to the highest average AR score
(the former was tested on a parameter grid of
`branching_factor in [10, 50, 100]`
and `threshold in [0.005, 0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1.0]`
and the latter on `affinity in ["rbf", "laplacian", "poly", "sigmoid"]`
and `gamma in [0.25, 0.5, 1.0, 2.5, 5.0]`).
Moreover, Gaussian mixtures used `n_init=100`.


```{python prepare_small,results="hide",echo=FALSE}
# We suggested that "parametric" datasets g2mg, h2mg should be studied separately.
# Subset: not g2mg, not h2mg
res2 = res.loc[~res.battery.isin(["g2mg", "h2mg"]), :]

# Subset: [large datasets]
res2 = res2.loc[res2.n<=10_000, :]

res2["dataset"] = res2["battery"] + "/" + res2["dataset"]



# For each dataset, method, compute maximal scores across
# all available true (reference) labels (if there alternative labellings
# of a given dataset):
res_max = res2.groupby(["dataset", "method"]).max().\
    reset_index().drop(["labels"], axis=1)
#res_max.head()
```



```{python plot_small,echo=FALSE,results="hide",warn=FALSE,fig.cap="Distribution of the AR index for each algorithm (small datasets); best=1.0.",fig.height=5.9375}
do_plot(res_max)
```

The Genie algorithm with `gini_threshold` of 0.3 gives the highest average
and median AR index and, at the same time, is subject to the least variability.
The (parametric!) EM algorithm fitting mixtures of Gaussians and the (perhaps lesser-known)
information-theoretic [ITM](https://github.com/amueller/information-theoretic-mst)
{cite}`itm` method (which is also based on a minimum spanning tree;
compare {cite}`cvimst`)
tend to output good quality outcomes as well.


Descriptive statistics for the ranks (for each dataset,
each algorithm that gets the highest AR index rounded to 2 decimal digits,
gets a rank of 1); lower ranks are better:

```{python ranks_small,echo=FALSE,results="asis"}
show_ranks(res_max)
```


## Large Datasets

Below we provide the results for the larger datasets (70,000-105,600 points).

```{python prepare_large,results="hide",echo=FALSE}
# We suggested that "parametric" datasets g2mg, h2mg should be studied separately.
# Subset: not g2mg, not h2mg
res2 = res.loc[~res.battery.isin(["g2mg", "h2mg"]), :]

# Subset: [large datasets]
res2 = res2.loc[res2.n>10_000, :]

res2["dataset"] = res2["battery"] + "/" + res2["dataset"]



# For each dataset, method, compute maximal scores across
# all available true (reference) labels (if there alternative labellings
# of a given dataset):
res_max = res2.groupby(["dataset", "method"]).max().\
    reset_index().drop(["labels"], axis=1)
#res_max.head()
```




```{python plot_large,echo=FALSE,results="hide",warn=FALSE,fig.cap="Distribution of the AR index for each algorithm (large datasets); best=1.0.",fig.height=5.9375}
do_plot(res_max)
```

This time, the ITM method and Genie with `gini_threshold` of 0.1 give
the highest typical scores.


Descriptive statistics for the ranks (AR index):

```{python ranks_large,echo=FALSE,results="asis"}
show_ranks(res_max)
```



## Summary

Overall, the Genie algorithm often outperforms other algorithms considered
in this study, at least on this rich benchmark battery.
In {cite}`genieins`, based on a much smaller sample of reference datasets,
we have recommended using `gini_threshold=0.3`,
which is set as the default also in the `genieclust` package.
However, sometimes inspecting thresholds equal to 0.1 and 0.5 is worth a try.
Interestingly, the algorithm is quite stable in the sense that
small changes of this parameter should not affect the generated clusterings
in a significant way.
