Timings (How Fast Is It?)
=========================

In the :any:`previous section <benchmarks_ar>` we have demonstrated
that Genie generates *quality* partitions. Now the crucial question is:
does it do it quickly?

Genie will be compared against K-means from `scikit-learn <https://scikit-learn.org/>`_ version 0.23.1
(`sklearn.cluster.KMeans`) for different number of threads
(by default it uses all available resources;
note that the number of restarts, `n_init`, defaults to 10)
and hierarchical agglomerative algorithms
with the centroid, median, and Ward linkage implemented in the
`fastcluster <http://www.danifold.net/fastcluster.html>`_ package.



Genie, just like the single linkage, is based on a minimum spanning tree (MST) of the
pairwise distance graph of an input point set.
Given the MST (the slow part), Genie itself has :math:`O(n \sqrt{n})` time
and :math:`O(n)` memory complexity.
Generally, our parallelised implementation of a Jarník (Prim/Dijkstra)-like
method [2]_ will be called to compute an MST, which takes :math:`O(d n^2)` time.
However, `mlpack.emst <https://www.mlpack.org/>`_ [3]_ provides a very fast
alternative in the case of Euclidean spaces of (very) low dimensionality,
see [4]_ and the `mlpack_enabled` parameter, which is automatically used
for datasets with up to :math:`d=6` features.
Moreover, in the approximate method (`exact` = ``False``), we apply
the Kruskal algorithm on the near-neighbour graph determined
by `nmslib` [5]_. Albeit this only gives *some* sort of a spanning *forest*,
such a data structure :any:`turns out to be very suitable for our clustering task <benchmarks_approx>`\ .

All timings will be performed on a PC running GNU/Linux 5.4.0-40-generic #44-Ubuntu
SMP x86_64 kernel with an Intel(R) Core(TM) i7-9750H CPU @ 2.60GHz (12M cache, 6 cores, 12 threads)
and total memory of 16,242,084 kB.


<<timings-imports,results="hidden",echo=False>>=
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import os.path, glob, re
import scipy.stats
#from natsort import natsorted
#import genieclust
import sklearn.metrics
import seaborn as sns
import pweave
from tabulate import tabulate
np.set_printoptions(precision=3, threshold=50, edgeitems=50)
pd.set_option("min_rows", 200)
pd.set_option("max_columns", 20)
#pd.set_option("display.width", 200)
plt.style.use("bmh")
plt.rcParams.update({
    'font.size': 9,
    'font.family': 'sans-serif',
    'font.sans-serif': ['Ubuntu Condensed', 'Alegreya', 'Alegreya Sans']})


res  = pd.read_csv("v1-timings.csv") # see timings.py
dims = pd.read_csv("v1-dims.csv")
dims["dataset"] = dims["battery"]+"/"+dims["dataset"]
dims = dims.loc[:,"dataset":]

#res = res.loc[res.method.isin([
#    "Genie_G0.1", "Genie_G0.3", "Genie_G0.5", "Genie_G1.0", "ITM",
#    "fastcluster_complete", "fastcluster_centroid", "fastcluster_average",
#    "fastcluster_ward", "sklearn_kmeans", "sklearn_gm", "sklearn_spectral_Arbf_G5",
#    "sklearn_birch_T0.01_BF100"]), :]
#


#res["method"] = res["method"].map({
#    "Genie_G0.1": "Genie_0.1",
#    "Genie_G0.3": "Genie_0.3",
#    "Genie_G1.0": "single",
#    "ITM": "ITM",
#    "Genie_G0.5": "Genie_0.5",
#    "fastcluster_complete": "complete",
#    "fastcluster_average": "average",
#    "fastcluster_centroid": "centroid",
#    "fastcluster_ward": "ward",
#    "sklearn_kmeans": "kmeans",
#    "sklearn_gm": "gauss_mix",
#    "sklearn_spectral_Arbf_G5": "spectral_rbf_5",
#    "sklearn_birch_T0.01_BF100": "birch_0.01",
#    })
@




Large Datasets
--------------


Let's study the algorithm's run times for some of the
"larger" datasets (70,000-105,600 observations,
see section on :any:`benchmark results <benchmarks_ar>` for discussion)
from the
`Benchmark Suite for Clustering Algorithms — Version 1 <https://github.com/gagolews/clustering_benchmarks_v1>`_ [1]_.
Features with variance of 0 were removed,
datasets were centred at **0** and scaled so that they have total variance of 1.
Tiny bit of Gaussian noise was added to each observation.
Clustering is performed with respect to the Euclidean distance.



<<timings-get-min,results="hidden",echo=False>>=
res2 = res.loc[(res.n_threads>0), "dataset":]
res2 = res2.loc[res.dataset.isin(["mnist/digits", "mnist/fashion",
    "sipu/worms_2", "sipu/worms_64"]), :]
res2 = res2.groupby(["dataset", "method", "n_clusters", "n_threads"]).\
    elapsed_time.min().reset_index()
res2 = pd.merge(res2, dims, on="dataset")
# what's missing:
# pd.set_option("display.width", 200)
# res.groupby(["dataset", "method", "n_clusters", "n_threads"]).size().unstack([3,2])
@




Here are the results (in seconds) if 6 threads are requested
(except for `fastcluster` which is not parallelised).
For K-means, the timings are listed as a function of the number of clusters to detect,
for the other hierarchical methods the run-times are almost identical irrespective of the
partitions' cardinality.

<<timings-summary,results="rst",echo=False>>=
_dat = res2.loc[(res2.n_threads==6) | res2.method.isin(["fastcluster_median", "fastcluster_centroid", "fastcluster_ward"]), \
    ["dataset","n", "d", "method","n_clusters","elapsed_time"]].\
set_index(["dataset","n", "d", "method","n_clusters"]).unstack().reset_index()
_dat = _dat.round(2)
_dat.columns = [l0 if not l1 else l1 for l0, l1 in _dat.columns]
_dat.loc[~_dat.method.isin(["sklearn_kmeans"]), 100] = np.nan
_dat.loc[~_dat.method.isin(["sklearn_kmeans"]), 1000] = np.nan
which_repeated = (_dat.dataset.shift(1) == _dat.dataset)
_dat.loc[which_repeated, "dataset"] = ""
_dat.loc[which_repeated, "n"] = ""
_dat.loc[which_repeated, "d"] = ""
_dat = tabulate(_dat, _dat.columns, tablefmt="rst", showindex=False)
_dat = _dat.replace("nan", "")
print(_dat, "\n\n")
@


Of course, the K-means algorithm is the fastest.
However, its performance degrades as K increases. Hence, it might not be
a good choice for the so-called *extreme clustering* problems.
Most importantly, the approximate version of Genie (based on `nmslib`)
is only slightly slower.
The exact variant is extremely performant in Euclidean spaces of low dimensionality
(thanks to `mlpack`) and overall at least 10 times more efficient than the other
hierarchical algorithms in this study.





Timings as a Function of `n` and `d`
------------------------------------

In order to study the run-times as a function dataset size and dimensionality,
let's consider a series of synthetic benchmarks, each with two Gaussian blobs of size `n/2`
(with i.i.d. coordinates), in a `d`-dimensional space.

Here are the medians of 3-10 timings (depending on the dataset size), in seconds,
on 6 threads:

<<g2mg-summary,results="rst",echo=False>>=
g2mg  = pd.read_csv("v1-g2mg.csv") # see timings_g2mg.py
# What's missing:
# g2mg.loc[g2mg.n_threads>0,:].groupby(["method", "n", "d"])[["elapsed_time"]].size().unstack(0)
# g2mg.loc[g2mg.n_threads>0,:].groupby(["method", "n", "d"])[["elapsed_time"]].agg(scipy.stats.variation).unstack(0)
# g2mg.loc[g2mg.n_threads>0,:].groupby(["method", "n", "d"])[["elapsed_time"]].median().unstack(0)
_dat = g2mg.loc[g2mg.method.isin(["Genie_0.3_approx", "Genie_0.3_nomlpack", "Genie_0.3_mlpack"]) & (g2mg.n.isin([10_000, 50_000, 100_000, 500_000, 1_000_000])),["method","n","d","elapsed_time"]].groupby(["method","n","d"]).median().reset_index()
_dat = _dat.set_index(["method", "d", "n"]).unstack().round(2).reset_index()
_dat.columns = [l0 if not l1 else l1 for l0, l1 in _dat.columns]
which_repeated = (_dat.method.shift(1) == _dat.method)
_dat.loc[which_repeated, "method"] = ""
print(tabulate(_dat, _dat.columns, tablefmt="rst", showindex=False), "\n\n")
@

By default, `mlpack_enabled` is ``"auto"``, which translates
to ``True`` if the requested metric is Euclidean,  Python package `mlpack` is available,
and `d` is not greater than 6.
The effect of the curse of dimensionality is clearly visible -- clustering
in very low-dimensional Euclidean spaces is extremely fast.
On the other hand, the approximate version of Genie can easily cluster
very large datasets. Only the system's memory limits might become a problem then.


<<g2mg-plot,results="hidden",echo=False,caption="Timings [s] as a function of the dataset size and dimensionality — problem sizes that can be solved during a coffee-break.">>=
_dat = g2mg.loc[g2mg.method.isin(["Genie_0.3_approx", "Genie_0.3_nomlpack", "Genie_0.3_mlpack"])&(g2mg.d>10),["method","n","d","elapsed_time"]].groupby(["method","n","d"]).median().reset_index()
sns.lineplot(x="n", y="elapsed_time", hue="method", style="d", data=_dat, markers=True)
#plt.yscale("log")
#plt.xscale("log")
plt.ylim(0, 600)
plt.show()
@





Timings as a Function of the Number of Threads
----------------------------------------------

Recall that the timings are done on a PC with 6 physical cores.
Genie turns out to be nicely parallelisable — as evidenced on
the ``mnist/digits`` dataset:


<<timings-plot,results="hidden",echo=False,caption="Timings [s] as a function of the number of clusters and threads.">>=
dataset="mnist/digits"
sns.lineplot(x="n_clusters", y="elapsed_time", hue="method", style="n_threads",
    data=res2.loc[(res2.dataset==dataset) & (res2.method.isin(["sklearn_kmeans", "Genie_0.3", "Genie_0.3_approx"])),:], markers=True)
plt.title("%s (n=%d, d=%d)" %(dataset, dims.loc[dims.dataset==dataset,"n"], dims.loc[dims.dataset==dataset,"d"]))
plt.xscale("log")
#plt.yscale("log")
plt.ylim(0, 2000)
plt.show()
@







Summary
-------

The approximate (`exact` = ``False``) version of Genie is much faster
than the original one. At the same time, it is still
:any:`highly compatible <benchmarks_approx>`\ with it
(at least at higher levels of the cluster hierarchy). Therefore, we
can safely recommend its use in large problem instances.
Most importantly, its performance is not much worse than the K-means method
with small K. Once a complete cluster hierarchy is determined,
partitioning of any cardinality can be extracted in less than 0.34 s on a 1M dataset.
Still, even the exact Genie is amongst the fastest clustering algorithms in the pool.

On top of that, we are also allowed to change our mind about the `gini_threshold`
parameter once the clustering is has been determined. The MST is stored for further
reference and is not recomputed unless needed. Here are the timings for
a first run of the algorithm:

<<timings-cache-1>>=
import time, genieclust, numpy as np
X = np.loadtxt("worms_2.data.gz", ndmin=2)
g = genieclust.Genie(n_clusters=2, gini_threshold=0.3)
t0 = time.time()
g.fit(X)
print("time elapsed - first run: %.3f" % (time.time()-t0))
@


Changing some parameters and re-running the cluster search:

<<timings-cache-2>>=
g.set_params(n_clusters=10)
g.set_params(gini_threshold=0.1)
t0 = time.time()
g.fit(X)
print("time elapsed - consecutive run: %.3f" % (time.time()-t0))
@


References
----------

.. [1]
    Gagolewski M., Cena A. (Eds.), *Benchmark Suite for Clustering Algorithms — Version 1*,
    2020. https://github.com/gagolews/clustering_benchmarks_v1. doi:10.5281/zenodo.3815066.

.. [2]
    Olson C.F., Parallel algorithms for hierarchical clustering,
    *Parallel Computing* 21(8), 1995, 1313-1325.
    doi:10.1016/0167-8191(95)00017-I.

.. [3]
    Curtin R.R., Edel M., Lozhnikov M., Mentekidis Y., Ghaisas S., Zhang S.,
    mlpack 3: A fast, flexible machine learning library,
    *Journal of Open Source Software* 3(26), 726, 2018.
    doi:10.21105/joss.00726.

.. [4]
    March W.B., Ram P., Gray A.G.,
    Fast Euclidean Minimum Spanning Tree: Algorithm, Analysis, and Applications,
    *Proc. ACM SIGKDD'10*, 2010, 603-611.

.. [5]
    Naidan B., Boytsov L., Malkov Y.,  Novak D.,
    *Non-metric space library (NMSLIB) manual*, version 2.0, 2019.
    https://github.com/nmslib/nmslib/blob/master/manual/latex/manual.pdf.
