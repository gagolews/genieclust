Benchmarks — Detailed Results
=============================

In one of the :any:`above sections <benchmarks_ar>`
we have summarised the AR indices based on the datasets from
the `Benchmark Suite for Clustering Algorithms – Version 1 <https://github.com/gagolews/clustering_benchmarks_v1>`_
:cite:`clustering_benchmarks_v1`.
In this section we present more detailed results for
some other partition similarity measures implemented in the `genieclust`
package — Fowlkes–Mallows :cite:`fm`, adjusted Rand :cite:`comparing_partitions`,
adjusted and normalised mutual information :cite:`nmi`,
normalised accuracy (purity) and pair sets index :cite:`psi`,
see the API documentation of :mod:`genieclust.compare_partitions` for more details.
In each case, score of 1.0 denotes perfect agreement between the clustering
results and the reference partitions.

At the preprocessing stage, features with variance of 0 were removed.
Every dataset has been centred at **0** and scaled so that is has total
variance of 1. Then, a tiny bit of Gaussian noise has been added to each
item. Clustering is performed with respect to the Euclidean distance
(wherever applicable).

All raw results can be found `here <https://github.com/gagolews/clustering_results_v1/>`_.

<<imports,results="hidden",echo=False>>=
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import os.path, glob, re
import scipy.stats
#from natsort import natsorted
#import genieclust
import sklearn.metrics
import seaborn as sns
import pweave
from tabulate import tabulate
np.set_printoptions(precision=3, threshold=50, edgeitems=50)
pd.set_option("min_rows", 200)
plt.style.use("bmh")
plt.rcParams.update({
    'font.size': 9,
    'font.family': 'sans-serif',
    'font.sans-serif': ['Ubuntu Condensed', 'Alegreya', 'Alegreya Sans']})

#``````````````````````````````````````````````````````````````````````````````
#``````````````````````````````````````````````````````````````````````````````
#``````````````````````````````````````````````````````````````````````````````


# Load results file:
res  = pd.read_csv("v1-scores.csv")
dims = pd.read_csv("v1-dims.csv")

# ari, afm can be negative --> replace negative indexes with 0.0
res.iloc[:,4:] = res.iloc[:,4:].transform(lambda x: np.maximum(x, 0.0), axis=1)

res = res.loc[res.method.isin([
    "Genie_G0.1", "Genie_G0.3", "Genie_G0.5", "Genie_G1.0", "ITM",
    "fastcluster_complete", "fastcluster_centroid", "fastcluster_average",
    "fastcluster_ward", "sklearn_kmeans", "sklearn_gm", "sklearn_spectral_Arbf_G5",
    "sklearn_birch_T0.01_BF100"]), :]

res["method"] = res["method"].map({
    "Genie_G0.1": "Genie_0.1",
    "Genie_G0.3": "Genie_0.3",
    "Genie_G1.0": "Single linkage",
    "ITM": "ITM",
    "Genie_G0.5": "Genie_0.5",
    "fastcluster_complete": "Complete linkage",
    "fastcluster_average": "Average linkage",
    "fastcluster_centroid": "Centroid linkage",
    "fastcluster_ward": "Ward linkage",
    "sklearn_kmeans": "K-means",
    "sklearn_gm": "Gaussian mixtures ",
    "sklearn_spectral_Arbf_G5": "Spectral_RBF_5",
    "sklearn_birch_T0.01_BF100": "Birch_0.01",
    })



def det(res_max):
    for similarity_measure in ["ar", "fm", "ami", "nmi", "nacc", "psi"]:
        print("%s\n%s\n" % (similarity_measure, "^"*len(similarity_measure)))

        print("Summary statistics for ``%s`` (best=1.0):\n\n" % similarity_measure)
        _dat = res_max.set_index(["dataset", "method"])[similarity_measure].unstack().\
        describe().T.round(2)
        print(tabulate(_dat, _dat.columns, tablefmt="rst"), "\n\n")

        print("Ranks for ``%s`` (best=1):\n\n" % similarity_measure)

        r = lambda x: scipy.stats.rankdata(-x, method="min")
        _dat = res_max.set_index(["dataset", "method"])[similarity_measure].unstack().\
        round(2).T.apply(r).T.describe().T.round(1)
        print(tabulate(_dat, _dat.columns, tablefmt="rst"), "\n\n")

        print("Raw results for ``%s`` (best=1.0):\n\n" % similarity_measure)

        _dat = res_max.set_index(["dataset", "method"])[similarity_measure].unstack().round(2)
        print(tabulate(_dat, _dat.columns, tablefmt="rst"), "\n\n")
@


Small Datasets
--------------


<<prepare_small,results="hidden",echo=False>>=
# We suggested that "parametric" datasets g2mg, h2mg should be studied separately.
# Subset: not g2mg, not h2mg
res2 = res.loc[~res.battery.isin(["g2mg", "h2mg"]), :]

# Subset: [large datasets]
res2 = res2.loc[res2.dataset.isin(dims.dataset[dims.n<=10_000]), :]

res2["dataset"] = res2["battery"] + "/" + res2["dataset"]



# For each dataset, method, compute maximal scores across
# all available true (reference) labels (if there alternative labellings
# of a given dataset):
res_max = res2.groupby(["dataset", "method"]).max().\
    reset_index().drop(["labels"], axis=1)
#res_max.head()
@




<<det_small,echo=False,results="rst",caption="">>=
det(res_max)
@


Summary
^^^^^^^

Medians and means of the partition similarity scores
(read row-wise, in groups of 2 columns):

<<indices_small,echo=False,results="hidden",caption="Heat map of median and mean similarity scores">>=
sns.heatmap(res_max.groupby("method")[["ar", "fm", "ami", "nmi", "nacc", "psi"]].\
aggregate([np.median, np.mean]), annot=True, vmin=0.5, vmax=1.0, fmt=".2f")
plt.yticks(rotation=0)
plt.xticks(rotation=45)
plt.xlabel('')
plt.ylabel('')
plt.show()
@




Large Datasets
--------------

<<prepare_large,results="hidden",echo=False>>=
# We suggested that "parametric" datasets g2mg, h2mg should be studied separately.
# Subset: not g2mg, not h2mg
res2 = res.loc[~res.battery.isin(["g2mg", "h2mg"]), :]

# Subset: [large datasets]
res2 = res2.loc[res2.dataset.isin(dims.dataset[dims.n>10_000]), :]

res2["dataset"] = res2["battery"] + "/" + res2["dataset"]



# For each dataset, method, compute maximal scores across
# all available true (reference) labels (if there alternative labellings
# of a given dataset):
res_max = res2.groupby(["dataset", "method"]).max().\
    reset_index().drop(["labels"], axis=1)
#res_max.head()
@


<<det_large,echo=False,results="rst",caption="">>=
det(res_max)
@



Summary
^^^^^^^

Medians and means of the partition similarity scores:

<<indices_large,echo=False,results="hidden",caption="Heat map of median and mean similarity scores">>=
sns.heatmap(res_max.groupby("method")[["ar", "fm", "ami", "nmi", "nacc", "psi"]].\
aggregate([np.median, np.mean]), annot=True, vmin=0.5, vmax=1.0, fmt=".2f")
plt.yticks(rotation=0)
plt.xticks(rotation=45)
plt.xlabel('')
plt.ylabel('')
plt.show()
@
