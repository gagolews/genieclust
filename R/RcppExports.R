# Generated by using Rcpp::compileAttributes() -> do not edit by hand
# Generator token: 10BE3573-1514-4C36-9D1C-5A225CD40393

#' @title External Cluster Validity Measures and Pairwise Partition Similarity Scores
#'
#' @description
#' The functions described in this section quantify the similarity between
#' two label vectors \code{x} and \code{y} which represent two partitions
#' of a set of \eqn{n} elements into, respectively, \eqn{K} and \eqn{L}
#' nonempty and pairwise disjoint subsets.
#'
#' For instance, \code{x} and \code{y} can represent two clusterings
#' of a dataset with \eqn{n} observations specified by two vectors
#' of labels. The functions described here can be used as external cluster
#' validity measures, where we assume that \code{x} is
#' a reference (ground-truth) partition whilst \code{y} is the vector
#' of predicted cluster memberships.
#'
#' All indices except \code{normalized_clustering_accuracy()}
#' can act as a pairwise partition similarity score: they are symmetric,
#' i.e., \code{index(x, y) == index(y, x)}.
#'
#' Each index except \code{mi_score()} (which computes the mutual
#' information score) outputs 1 given two identical partitions.
#' Note that partitions are always defined up to a permutation (bijection)
#' of the set of possible labels, e.g., (1, 1, 2, 1) and (4, 4, 2, 4)
#' represent the same 2-partition.
#'
#' @details
#' \code{normalized_clustering_accuracy()} (Gagolewski, 2023)
#' is an asymmetric external cluster validity measure
#' which assumes that the label vector \code{x} (or rows in the confusion
#' matrix) represents the reference (ground truth) partition.
#' It is an average proportion of correctly classified points in each cluster
#' above the worst case scenario of uniform membership assignment,
#' with cluster ID matching based on the solution to the maximal linear
#' sum assignment problem; see \code{\link{normalized_confusion_matrix}}).
#' It is given by:
#' \eqn{\max_\sigma \frac{1}{K} \sum_{j=1}^K \frac{c_{\sigma(j), j}-c_{\sigma(j),\cdot}/K}{c_{\sigma(j),\cdot}-c_{\sigma(j),\cdot}/K}},
#' where \eqn{C} is a confusion matrix with \eqn{K} rows and \eqn{L} columns,
#' \eqn{\sigma} is a permutation of the set \eqn{\{1,\dots,\max(K,L)\}}, and
#' \eqn{c_{i, \cdot}=c_{i, 1}+...+c_{i, L}} is the i-th row sum,
#' under the assumption that \eqn{c_{i,j}=0} for \eqn{i>K} or \eqn{j>L}
#' and \eqn{0/0=0}.
#'
#' \code{normalized_pivoted_accuracy()} is defined as
#' \eqn{(\max_\sigma \sum_{j=1}^{\max(K,L)} c_{\sigma(j),j}/n-1/\max(K,L))/(1-1/\max(K,L))},
#' where \eqn{\sigma} is a permutation of the set \eqn{\{1,\dots,\max(K,L)\}},
#' and \eqn{n} is the sum of all elements in \eqn{C}.
#' For non-square matrices, missing rows/columns are assumed
#' to be filled with 0s.
#'
#' \code{pair_sets_index()} (PSI) was introduced in (Rezaei, Franti, 2016).
#' The simplified PSI assumes E=1 in the definition of the index,
#' i.e., uses Eq. (20) in the said paper instead of Eq. (18).
#' For non-square matrices, missing rows/columns are assumed
#' to be filled with 0s.
#'
#' \code{rand_score()} gives the Rand score (the "probability" of agreement
#' between the two partitions) and
#' \code{adjusted_rand_score()} is its version corrected for chance,
#' see (Hubert, Arabie, 1985): its expected value is 0 given two independent
#' partitions. Due to the adjustment, the resulting index may be negative
#' for some inputs.
#'
#' Similarly, \code{fm_score()} gives the Fowlkes-Mallows (FM) score
#' and \code{adjusted_fm_score()} is its adjusted-for-chance version;
#' see (Hubert, Arabie, 1985).
#'
#' \code{mi_score()}, \code{adjusted_mi_score()} and
#' \code{normalized_mi_score()} are information-theoretic
#' scores, based on mutual information,
#' see the definition of \eqn{AMI_{sum}} and \eqn{NMI_{sum}}
#' in (Vinh et al., 2010).
#'
#'
#' \code{normalized_confusion_matrix()} computes the confusion matrix
#' and permutes its rows and columns so that the sum of the elements
#' of the main diagonal is the largest possible (by solving
#' the maximal assignment problem).
#' The function only accepts \eqn{K \leq L}.
#' The reordering of the columns of a confusion matrix can be determined
#' by calling \code{normalizing_permutation()}.
#'
#' Also note that the built-in
#' \code{\link{table}()} determines the standard confusion matrix.
#'
#'
#' @references
#' Gagolewski M., A framework for benchmarking clustering algorithms,
#' \emph{SoftwareX} 20, 2022, 101270,
#' \doi{10.1016/j.softx.2022.101270},
#' \url{https://clustering-benchmarks.gagolewski.com}.
#'
#' Gagolewski M., Normalised clustering accuracy: An asymmetric external
#' cluster validity measure, \emph{Journal of Classification} 42, 2025, 2-30.
#' \doi{10.1007/s00357-024-09482-2}.
#'
#' Hubert L., Arabie P., Comparing partitions,
#' \emph{Journal of Classification} 2(1), 1985, 193-218, esp. Eqs. (2) and (4).
#'
#' Meila M., Heckerman D., An experimental comparison of model-based clustering
#' methods, \emph{Machine Learning} 42, 2001, pp. 9-29,
#' \doi{10.1023/A:1007648401407}.
#'
#' Rezaei M., Franti P., Set matching measures for external cluster validity,
#' \emph{IEEE Transactions on Knowledge and Data Mining} 28(8), 2016,
#' 2173-2186.
#'
#' Steinley D., Properties of the Hubert-Arabie adjusted Rand index,
#' \emph{Psychological Methods} 9(3), 2004, pp. 386-396,
#' \doi{10.1037/1082-989X.9.3.386}.
#'
#' Vinh N.X., Epps J., Bailey J.,
#' Information theoretic measures for clusterings comparison:
#' Variants, properties, normalization and correction for chance,
#' \emph{Journal of Machine Learning Research} 11, 2010, 2837-2854.
#'
#'
#'
#' @param x an integer vector of length n (or an object coercible to)
#' representing a K-partition of an n-set (e.g., a reference partition),
#' or a confusion matrix with K rows and L columns
#' (see \code{\link{table}(x, y)})
#'
#' @param y an integer vector of length n (or an object coercible to)
#' representing an L-partition of the same set (e.g., the output of a
#' clustering algorithm we wish to compare with \code{x}),
#' or NULL (if x is an K*L confusion matrix)
#'
#' @param simplified whether to assume E=1 in the definition of the pair sets index index,
#'     i.e., use Eq. (20) in (Rezaei, Franti, 2016) instead of Eq. (18)
#'
#' @param clipped whether the result should be clipped to the unit interval, i.e., [0, 1]
#'
#'
#' @return Each cluster validity measure is a single numeric value.
#'
#' \code{normalized_confusion_matrix()} returns a numeric matrix.
#'
#' \code{normalizing_permutation()} returns a vector of indexes.
#'
#'
#' @examples
#' y_true <- iris[[5]]
#' y_pred <- kmeans(as.matrix(iris[1:4]), 3)$cluster
#' normalized_clustering_accuracy(y_true, y_pred)
#' normalized_pivoted_accuracy(y_true, y_pred)
#' pair_sets_index(y_true, y_pred)
#' pair_sets_index(y_true, y_pred, simplified=TRUE)
#' adjusted_rand_score(y_true, y_pred)
#' rand_score(table(y_true, y_pred)) # the same
#' adjusted_fm_score(y_true, y_pred)
#' fm_score(y_true, y_pred)
#' mi_score(y_true, y_pred)
#' normalized_mi_score(y_true, y_pred)
#' adjusted_mi_score(y_true, y_pred)
#' normalized_confusion_matrix(y_true, y_pred)
#' normalizing_permutation(y_true, y_pred)
#'
#' @rdname compare_partitions
#' @name compare_partitions
#' @export
normalized_clustering_accuracy <- function(x, y = NULL) {
    .Call(`_genieclust_normalized_clustering_accuracy`, x, y)
}

#' @rdname compare_partitions
#' @export
normalized_pivoted_accuracy <- function(x, y = NULL) {
    .Call(`_genieclust_normalized_pivoted_accuracy`, x, y)
}

#' @rdname compare_partitions
#' @export
pair_sets_index <- function(x, y = NULL, simplified = FALSE, clipped = TRUE) {
    .Call(`_genieclust_pair_sets_index`, x, y, simplified, clipped)
}

#' @rdname compare_partitions
#' @export
adjusted_rand_score <- function(x, y = NULL, clipped = FALSE) {
    .Call(`_genieclust_adjusted_rand_score`, x, y, clipped)
}

#' @rdname compare_partitions
#' @export
rand_score <- function(x, y = NULL) {
    .Call(`_genieclust_rand_score`, x, y)
}

#' @rdname compare_partitions
#' @export
adjusted_fm_score <- function(x, y = NULL, clipped = FALSE) {
    .Call(`_genieclust_adjusted_fm_score`, x, y, clipped)
}

#' @rdname compare_partitions
#' @export
fm_score <- function(x, y = NULL) {
    .Call(`_genieclust_fm_score`, x, y)
}

#' @rdname compare_partitions
#' @export
mi_score <- function(x, y = NULL) {
    .Call(`_genieclust_mi_score`, x, y)
}

#' @rdname compare_partitions
#' @export
normalized_mi_score <- function(x, y = NULL) {
    .Call(`_genieclust_normalized_mi_score`, x, y)
}

#' @rdname compare_partitions
#' @export
adjusted_mi_score <- function(x, y = NULL, clipped = FALSE) {
    .Call(`_genieclust_adjusted_mi_score`, x, y, clipped)
}

#' @rdname compare_partitions
#' @export
normalized_confusion_matrix <- function(x, y = NULL) {
    .Call(`_genieclust_normalized_confusion_matrix`, x, y)
}

#' @rdname compare_partitions
#' @export
normalizing_permutation <- function(x, y = NULL) {
    .Call(`_genieclust_normalizing_permutation`, x, y)
}

#' @title Internal Cluster Validity Measures
#'
#' @description
#' Implementation of a number of so-called cluster validity indices critically
#' reviewed in (Gagolewski, Bartoszuk, Cena, 2021). See Section 2
#' therein and (Gagolewski, 2022) for the respective definitions.
#'
#' The greater the index value, the more \emph{valid} (whatever that means)
#' the assessed partition. For consistency, the Ball-Hall and
#' Davies-Bouldin indexes as well as the within-cluster sum of squares (WCSS)
#' take negative values.
#'
#'
#' @param X numeric matrix with \code{n} rows and \code{d} columns,
#'     representing \code{n} points in a \code{d}-dimensional space
#'
#' @param y vector of \code{n} integer labels,
#'     representing a partition whose \emph{quality} is to be
#'     assessed; \code{y[i]} is the cluster ID of the \code{i}-th point,
#'     \code{X[i, ]}; \code{1 <= y[i] <= K}, where \code{K} is the number
#'     or clusters
#'
#' @param M number of nearest neighbours
#'
#' @param lowercase_d an integer between 1 and 5, denoting
#'     \eqn{d_1}, ..., \eqn{d_5} in the definition
#'     of the generalised Dunn (Bezdek-Pal) index (numerator:
#'     min, max, and mean pairwise intracluster distance,
#'     distance between cluster centroids,
#'     weighted point-centroid distance, respectively)
#'
#' @param uppercase_d an integer between 1 and 3, denoting
#'     \eqn{D_1}, ..., \eqn{D_3} in the definition
#'     of the generalised Dunn (Bezdek-Pal) index (denominator:
#'       max and min pairwise intracluster distance, average point-centroid
#'       distance, respectively)
#'
#' @param owa_numerator,owa_denominator single string specifying
#'     the OWA operators to use in the definition of the DuNN index;
#'     one of: \code{"Mean"}, \code{"Min"}, \code{"Max"}, \code{"Const"},
#'     \code{"SMin:D"}, \code{"SMax:D"}, where \code{D} is an integer
#'     defining the degree of smoothness
#'
#'
#' @return
#' A single numeric value (the more, the \emph{better}).
#'
#' @references
#' Ball G.H., Hall D.J.,
#' \emph{ISODATA: A novel method of data analysis and pattern classification},
#' Technical report No. AD699616, Stanford Research Institute, 1965.
#'
#' Bezdek J., Pal N., Some new indexes of cluster validity,
#' \emph{IEEE Transactions on Systems, Man, and Cybernetics, Part B} 28,
#' 1998, 301-315, \doi{10.1109/3477.678624}.
#'
#' Calinski T., Harabasz J., A dendrite method for cluster analysis,
#' \emph{Communications in Statistics} 3(1), 1974, 1-27,
#' \doi{10.1080/03610927408827101}.
#'
#' Davies D.L., Bouldin D.W.,
#' A Cluster Separation Measure,
#' \emph{IEEE Transactions on Pattern Analysis and Machine Intelligence}
#' PAMI-1 (2), 1979, 224-227, \doi{10.1109/TPAMI.1979.4766909}.
#'
#' Dunn J.C., A Fuzzy Relative of the ISODATA Process and Its Use in Detecting
#' Compact Well-Separated Clusters, \emph{Journal of Cybernetics} 3(3), 1973,
#' 32-57, \doi{10.1080/01969727308546046}.
#'
#' Gagolewski M., Bartoszuk M., Cena A.,
#' Are cluster validity measures (in)valid?, \emph{Information Sciences} 581,
#' 620-636, 2021, \doi{10.1016/j.ins.2021.10.004};
#' preprint: \url{https://raw.githubusercontent.com/gagolews/bibliography/master/preprints/2021cvi.pdf}.
#'
#' Gagolewski M., A Framework for Benchmarking Clustering Algorithms,
#' \emph{SoftwareX} 20, 2022, 101270,
#' \doi{10.1016/j.softx.2022.101270},
#' \url{https://clustering-benchmarks.gagolewski.com}.
#'
#' Rousseeuw P.J., Silhouettes: A Graphical Aid to the Interpretation and
#' Validation of Cluster Analysis, \emph{Computational and Applied Mathematics}
#' 20, 1987, 53-65, \doi{10.1016/0377-0427(87)90125-7}.
#'
#'
#'
#' @examples
#' X <- as.matrix(iris[,1:4])
#' X[,] <- jitter(X)  # otherwise we get a non-unique solution
#' y <- as.integer(iris[[5]])
#' calinski_harabasz_index(X, y)  # good
#' calinski_harabasz_index(X, sample(1:3, nrow(X), replace=TRUE))  # bad
#'
#' @name cluster_validity
#' @rdname cluster_validity
#' @export
calinski_harabasz_index <- function(X, y) {
    .Call(`_genieclust_calinski_harabasz_index`, X, y)
}

#' @rdname cluster_validity
#' @export
dunnowa_index <- function(X, y, M = 25L, owa_numerator = "SMin:5", owa_denominator = "Const") {
    .Call(`_genieclust_dunnowa_index`, X, y, M, owa_numerator, owa_denominator)
}

#' @rdname cluster_validity
#' @export
generalised_dunn_index <- function(X, y, lowercase_d, uppercase_d) {
    .Call(`_genieclust_generalised_dunn_index`, X, y, lowercase_d, uppercase_d)
}

#' @rdname cluster_validity
#' @export
negated_ball_hall_index <- function(X, y) {
    .Call(`_genieclust_negated_ball_hall_index`, X, y)
}

#' @rdname cluster_validity
#' @export
negated_davies_bouldin_index <- function(X, y) {
    .Call(`_genieclust_negated_davies_bouldin_index`, X, y)
}

#' @rdname cluster_validity
#' @export
negated_wcss_index <- function(X, y) {
    .Call(`_genieclust_negated_wcss_index`, X, y)
}

#' @rdname cluster_validity
#' @export
silhouette_index <- function(X, y) {
    .Call(`_genieclust_silhouette_index`, X, y)
}

#' @rdname cluster_validity
#' @export
silhouette_w_index <- function(X, y) {
    .Call(`_genieclust_silhouette_w_index`, X, y)
}

#' @rdname cluster_validity
#' @export
wcnn_index <- function(X, y, M = 25L) {
    .Call(`_genieclust_wcnn_index`, X, y, M)
}

#' @title
#' Get or Set the Number of Threads
#'
#' @description
#' These functions get or set the maximal number of OpenMP threads that
#' can be used by \code{\link{knn_euclid}} and \code{\link{mst_euclid}},
#' amongst others.
#'
#' @param n_threads maximum number of threads to use;
#'
#' @return
#' \code{omp_get_max_threads} returns the maximal number
#' of threads that will be used during the next call to a parallelised
#' function, not the maximal number of threads possibly available.
#' It there is no built-in support for OpenMP, 1 is always returned.
#'
#' For \code{omp_set_num_threads}, the previous value of \code{max_threads}
#' is output.
#'
#'
#' @rdname omp
#' @export
omp_set_num_threads <- function(n_threads) {
    .Call(`_genieclust_Romp_set_num_threads`, n_threads)
}

#' @rdname omp
#' @export
omp_get_max_threads <- function() {
    .Call(`_genieclust_Romp_get_max_threads`)
}

#' @title Quite Fast Euclidean Nearest Neighbours
#'
#' @description
#' If \code{Y} is \code{NULL}, then the function determines the first \code{k}
#' amongst the nearest neighbours of each point in \code{X} with respect
#' to the Euclidean distance. It is assumed that each query point is
#' not its own neighbour.
#'
#' Otherwise, for each point in \code{Y}, this function determines the \code{k}
#' nearest points thereto from \code{X}.
#'
#' @details
#' The implemented algorithms, see the \code{algorithm} parameter, assume
#' that \code{k} is rather small; say, \code{k <= 20}.
#'
#' Our implementation of K-d trees (Bentley, 1975) has been quite optimised;
#' amongst others, it has good locality of reference, features the sliding
#' midpoint (midrange) rule suggested by Maneewongvatana and Mound (1999),
#' and a node pruning strategy inspired by the discussion
#' by Sample et al. (2001).  Still, it is well-known that K-d trees
#' perform well only in spaces of low intrinsic dimensionality.  Thus,
#' due to the so-called curse of dimensionality, for high \code{d},
#' the brute-force algorithm is recommended.
#'
#' The number of threads used is controlled via the \code{OMP_NUM_THREADS}
#' environment variable or via the \code{\link{omp_set_num_threads}} function
#' at runtime. For best speed, consider building the package
#' from sources using, e.g., \code{-O3 -march=native} compiler flags.
#'
#'
#' @references
#' J.L. Bentley, Multidimensional binary search trees used for associative
#' searching, \emph{Communications of the ACM} 18(9), 509–517, 1975,
#' \doi{10.1145/361002.361007}.
#'
#' S. Maneewongvatana, D.M. Mount, It's okay to be skinny, if your friends
#' are fat, \emph{4th CGC Workshop on Computational Geometry}, 1999.
#'
#' N. Sample, M. Haines, M. Arnold, T. Purcell, Optimizing search
#' strategies in K-d Trees, \emph{5th WSES/IEEE Conf. on Circuits, Systems,
#' Communications & Computers} (CSCC'01), 2001.
#'
#'
#' @param X the "database"; a matrix of shape (n,d)
#' @param k number of nearest neighbours (should be rather small, say, <= 20)
#' @param Y the "query points"; \code{NULL} or a matrix of shape (m,d);
#'     note that setting \code{Y=X}, contrary to \code{NULL},
#'     will include the query points themselves amongst their own neighbours
#' @param algorithm \code{"auto"}, \code{"kd_tree"} or \code{"brute"};
#'     K-d trees can only be used for d between 2 and 20 only;
#'     \code{"auto"} selects \code{"kd_tree"} in low-dimensional spaces
#' @param max_leaf_size maximal number of points in the K-d tree leaves;
#'        smaller leaves use more memory, yet are not necessarily faster;
#'        use \code{0} to select the default value, currently set to 32
#' @param squared whether to return the squared Euclidean distance
#' @param verbose whether to print diagnostic messages
#'
#'
#' @return
#' A list with two elements, \code{nn.index} and \code{nn.dist}.
#'
#' \code{nn.dist} has shape (n,k) or (m,k);
#' \code{nn.dist[i,]} is sorted nondecreasingly for all \code{i}.
#' \code{nn.dist[i,j]} gives the weight of the edge \code{{i, ind[i,j]}},
#' i.e., the distance between the \code{i}-th point and its \code{j}-th NN.
#'
#' \code{nn.index} is of the same shape.
#' \code{nn.index[i,j]} is the index (between \code{1} and \code{n})
#' of the \code{j}-th nearest neighbour of \code{i}.
#'
#'
#' @examples
#' library("datasets")
#' data("iris")
#' X <- jitter(as.matrix(iris[1:2]))  # some data
#' neighbours <- knn_euclid(X, 1)  # 1-NNs of each point
#' plot(X, asp=1, las=1)
#' segments(X[,1], X[,2], X[neighbours$nn.index,1], X[neighbours$nn.index,2])
#'
#' knn_euclid(X, 5, matrix(c(6, 4), nrow=1))  # five closest points to (6, 4)
#'
#'
#' @seealso \code{\link{mst_euclid}}
#'
#' @rdname fastknn
#' @export
knn_euclid <- function(X, k = 1L, Y = NULL, algorithm = "auto", max_leaf_size = 0L, squared = FALSE, verbose = FALSE) {
    .Call(`_genieclust_knn_euclid`, X, k, Y, algorithm, max_leaf_size, squared, verbose)
}

#' @title Quite Fast Euclidean Minimum Spanning Trees (Also WRT Mutual Reachability Distances)
#'
#' @description
#' The function determines the/a(*) minimum spanning tree (MST) of a set
#' of \eqn{n} points, i.e., an acyclic undirected graph whose vertices represent
#' the points, and \eqn{n-1} edges with the minimal sum of weights, given by
#' the pairwise distances.  MSTs have many uses in, amongst others,
#' topological data analysis (clustering, dimensionality reduction, etc.).
#'
#' For \eqn{M\leq 2}, we get a spanning tree that minimises the sum of Euclidean
#' distances between the points. If \eqn{M=2}, the function additionally returns
#' the distance to each point's nearest neighbour.
#'
#' If \eqn{M>2}, the spanning tree is the smallest wrt the degree-M
#' mutual reachability distance (Campello et al., 2013) given by
#' \eqn{d_M(i, j)=\max\{ c_M(i), c_M(j), d(i, j)\}}, where \eqn{d(i,j)}
#' is the Euclidean distance between the \eqn{i}-th and the \eqn{j}-th point,
#' and \eqn{c_M(i)} is the \eqn{i}-th \eqn{M}-core distance defined as the distance
#' between the \eqn{i}-th point and its \eqn{(M-1)}-th nearest neighbour
#' (not including the query points themselves).
#' In clustering and density estimation, M plays the role of a smoothing
#' factor; see (Campello et al. 2015) and the references therein for discussion.
#'
#'
#' @details
#' (*) We note that if there are many pairs of equidistant points,
#' there can be many minimum spanning trees. In particular, it is likely
#' that there are point pairs with the same mutual reachability distances.
#' To make the definition less ambiguous (albeit with no guarantees),
#' internally, the brute-force algorithm relies on the adjusted distance
#' \eqn{d_M(i, j)=\max\{c_M(i), c_M(j), d(i, j)\}+\varepsilon d(i, j)},
#' where \eqn{\varepsilon} is close to 0, see \code{dcore_dist_adj}.
#' For the K-d tree-based methods, on the other hand, negative
#' \code{dcore_dist_adj} indicates the preference towards connecting to
#' farther points wrt the original metric in the case of the same
#' core distance instead of closer ones if the adjustment is positive.
#' When preferring farther points, the resulting spanning tree tends to
#' have more leaves.
#'
#' The implemented algorithms, see the \code{algorithm} parameter, assume
#' that \code{M} is rather small; say, \eqn{M \leq 20}.
#'
#' Our implementation of K-d trees (Bentley, 1975) has been quite optimised;
#' amongst others, it has good locality of reference (at the cost of making
#' a copy of the input dataset), features the sliding
#' midpoint (midrange) rule suggested by Maneewongvatana and Mound (1999),
#' and a node pruning strategy inspired by the discussion
#' by Sample et al. (2001).
#'
#' The "single-tree" version of the Borůvka algorithm is naively
#' parallelisable: in every iteration, it seeks each point's nearest "alien",
#' i.e., the nearest point thereto from another cluster.
#' The "dual-tree" Borůvka version of the algorithm is, in principle, based
#' on (March et al., 2010). As far as our implementation is concerned,
#' the dual-tree approach is often only faster in 2- and 3-dimensional spaces,
#' for \eqn{M\leq 2}, and in a single-threaded setting.  For another (approximate)
#' adaptation of the dual-tree algorithm to the mutual reachability distance;
#' see (McInnes and Healy, 2017).
#'
#' Nevertheless, it is well-known that K-d trees perform well only in spaces
#' of low intrinsic dimensionality (a.k.a. the "curse").  For high \code{d},
#' the "brute-force" algorithm is recommended.  Here, we provided a
#' parallelised (see Olson, 1995) version of the Jarník (1930) (a.k.a.
#' Prim (1957) or Dijkstra) algorithm, where the distances are computed
#' on the fly (only once for \code{M<=2}).
#'
#' The number of threads used is controlled via the \code{OMP_NUM_THREADS}
#' environment variable or via the \code{\link{omp_set_num_threads}} function
#' at runtime. For best speed, consider building the package
#' from sources using, e.g., \code{-O3 -march=native} compiler flags.
#'
#'
#' @references
#' V. Jarník, O jistém problému minimálním,
#' \emph{Práce Moravské Přírodovědecké Společnosti} 6, 1930, 57–63.
#'
#' C.F. Olson, Parallel algorithms for hierarchical clustering,
#' Parallel Computing 21(8), 1995, 1313–1325.
#'
#' R. Prim, Shortest connection networks and some generalizations,
#' \emph{The Bell System Technical Journal} 36(6), 1957, 1389–1401.
#'
#' O. Borůvka, O jistém problému minimálním, \emph{Práce Moravské
#' Přírodovědecké Společnosti} 3, 1926, 37–58.
#'
#' W.B. March, R. Parikshit, A.G. Gray, Fast Euclidean minimum spanning
#' tree: Algorithm, analysis, and applications, \emph{Proc. 16th ACM SIGKDD
#' Intl. Conf. Knowledge Discovery and Data Mining (KDD '10)}, 2010, 603–612.
#'
#' J.L. Bentley, Multidimensional binary search trees used for associative
#' searching, \emph{Communications of the ACM} 18(9), 509–517, 1975,
#' \doi{10.1145/361002.361007}.
#'
#' S. Maneewongvatana, D.M. Mount, It's okay to be skinny, if your friends
#' are fat, \emph{4th CGC Workshop on Computational Geometry}, 1999.
#'
#' N. Sample, M. Haines, M. Arnold, T. Purcell, Optimizing search
#' strategies in K-d Trees, \emph{5th WSES/IEEE Conf. on Circuits, Systems,
#' Communications & Computers} (CSCC'01), 2001.
#'
#' R.J.G.B. Campello, D. Moulavi, J. Sander, Density-based clustering based
#' on hierarchical density estimates, \emph{Lecture Notes in Computer Science}
#' 7819, 2013, 160–172. \doi{10.1007/978-3-642-37456-2_14}.
#'
#' R.J.G.B. Campello, D. Moulavi, A. Zimek. J. Sander, Hierarchical
#' density estimates for data clustering, visualization, and outlier detection,
#' \emph{ACM Transactions on Knowledge Discovery from Data (TKDD)} 10(1),
#' 2015, 1–51, \doi{10.1145/2733381}.
#'
#' L. McInnes, J. Healy, Accelerated hierarchical density-based
#' clustering, \emph{IEEE Intl. Conf. Data Mining Workshops (ICMDW)}, 2017,
#' 33–42, \doi{10.1109/ICDMW.2017.12}.
#'
#'
#'
#' @param X the "database"; a matrix of shape (n,d)
#' @param M the degree of the mutual reachability distance
#'          (should be rather small, say, \eqn{\leq 20}).
#'          \eqn{M\leq 2} denotes the ordinary Euclidean distance
#' @param algorithm \code{"auto"}, \code{"kd_tree_single"}
#'          \code{"kd_tree_dual"} or \code{"brute"};
#'     K-d trees can only be used for d between 2 and 20 only;
#'     \code{"auto"} selects \code{"kd_tree_dual"} for \eqn{d\leq 3},
#'     \eqn{M\leq 2}, and in a single-threaded setting only.
#'     \code{"kd_tree_single"} is used otherwise, unless \eqn{d>20}.
#' @param max_leaf_size maximal number of points in the K-d tree leaves;
#'        smaller leaves use more memory, yet are not necessarily faster;
#'        use \code{0} to select the default value, currently set to 32 for the
#'        single-tree and 8 for the dual-tree Boruvka algorithm
#' @param first_pass_max_brute_size minimal number of points in a node to
#'        treat it as a leaf (unless it's actually a leaf) in the first
#'        iteration of the algorithm; use \code{0} to select the default value,
#'        currently set to 32
#' @param dcore_dist_adj mutual reachability distance adjustment,
#'        a constant close to 0; in the case of ambiguity caused by equal
#'        core distances, a negative value will prefer connecting to farther
#'        points wrt the original distance, and closer ones in the case of
#'        a positive value
#' @param verbose whether to print diagnostic messages
#'
#'
#' @return
#' A list with two (M=1) or four (M>1) elements, \code{mst.index} and
#' \code{mst.dist}, and additionally \code{nn.index} and \code{nn.dist}.
#'
#' \code{mst.index} is a matrix with \eqn{n-1} rows and \code{2} columns,
#' whose rows define the tree edges.
#'
#' \code{mst.dist} is a vector of length
#' \code{n-1} giving the weights of the corresponding edges.
#'
#' The tree edges are ordered w.r.t. weights nondecreasingly, and then by
#' the indexes (lexicographic ordering of the \code{(weight, index1, index2)}
#' triples).  For each \code{i}, it holds \code{mst_ind[i,1]<mst_ind[i,2]}.
#'
#' \code{nn.index} is an \code{n} by \code{M-1} matrix giving the indexes
#' of each point's nearest neighbours. \code{nn.dist} provides the
#' corresponding distances.
#'
#'
#' @examples
#' library("datasets")
#' data("iris")
#' X <- jitter(as.matrix(iris[1:2]))  # some data
#' T <- mst_euclid(X)                 # Euclidean MST of X
#' plot(X, asp=1, las=1)
#' segments(X[T$mst.index[, 1], 1], X[T$mst.index[, 1], 2],
#'          X[T$mst.index[, 2], 1], X[T$mst.index[, 2], 2])
#'
#' @seealso \code{\link{knn_euclid}}
#'
#' @rdname fastmst
#' @export
mst_euclid <- function(X, M = 1L, algorithm = "auto", max_leaf_size = 0L, first_pass_max_brute_size = 0L, dcore_dist_adj = -0.00000001490116119384765625, verbose = FALSE) {
    .Call(`_genieclust_mst_euclid`, X, M, algorithm, max_leaf_size, first_pass_max_brute_size, dcore_dist_adj, verbose)
}

.genie <- function(mst, k, gini_threshold, postprocess, detect_noise, verbose) {
    .Call(`_genieclust_dot_genie`, mst, k, gini_threshold, postprocess, detect_noise, verbose)
}

.gclust <- function(mst, gini_threshold, verbose) {
    .Call(`_genieclust_dot_gclust`, mst, gini_threshold, verbose)
}

#' @title Inequality Measures
#'
#' @description
#' \code{gini_index()} gives the normalised Gini index,
#' \code{bonferroni_index()} implements the Bonferroni index, and
#' \code{devergottini_index()} implements the De Vergottini index.
#'
#' @details
#' These indices can be used to quantify the "inequality" of a numeric sample.
#' They can be conceived as normalised measures of data dispersion.
#' For constant vectors (perfect equity), the indices yield values of 0.
#' Vectors with all elements but one equal to 0 (perfect inequality),
#' are assigned scores of 1.
#' They follow the Pigou-Dalton principle (are Schur-convex):
#' setting \eqn{x_i = x_i - h} and \eqn{x_j = x_j + h} with \eqn{h > 0}
#' and \eqn{x_i - h \geq  x_j + h} (taking from the "rich" and
#' giving to the "poor") decreases the inequality
#'
#' These indices have applications in economics, amongst others.
#' The Genie clustering algorithm uses the Gini index as a measure
#' of the inequality of cluster sizes.
#'
#'
#' The normalised Gini index is given by:
#' \deqn{
#'     G(x_1,\dots,x_n) = \frac{
#'     \sum_{i=1}^{n} (n-2i+1) x_{\sigma(n-i+1)}
#'     }{
#'     (n-1) \sum_{i=1}^n x_i
#'     },
#' }
#'
#' The normalised Bonferroni index is given by:
#' \deqn{
#'     B(x_1,\dots,x_n) = \frac{
#'     \sum_{i=1}^{n}  (n-\sum_{j=1}^i \frac{n}{n-j+1})
#'          x_{\sigma(n-i+1)}
#'     }{
#'     (n-1) \sum_{i=1}^n x_i
#'     }.
#' }
#'
#' The normalised De Vergottini index is given by:
#' \deqn{
#'     V(x_1,\dots,x_n) =
#'     \frac{1}{\sum_{i=2}^n \frac{1}{i}} \left(
#'        \frac{ \sum_{i=1}^n \left( \sum_{j=i}^{n} \frac{1}{j}\right)
#'        x_{\sigma(n-i+1)} }{\sum_{i=1}^{n} x_i} - 1
#'     \right).
#' }
#'
#' Here, \eqn{\sigma} is an ordering permutation of \eqn{(x_1,\dots,x_n)}.
#'
#' Time complexity: \eqn{O(n)} for sorted (increasingly) data.
#' Otherwise, the vector will be sorted.
#'
#'
#' @references
#' Bonferroni C., \emph{Elementi di Statistica Generale}, Libreria Seber,
#' Firenze, 1930.
#'
#' Gagolewski M., Bartoszuk M., Cena A., Genie: A new, fast, and
#' outlier-resistant hierarchical clustering algorithm,
#' \emph{Information Sciences} 363, 2016, pp. 8-23.
#' \doi{10.1016/j.ins.2016.05.003}
#'
#' Gini C., \emph{Variabilita e Mutabilita},
#' Tipografia di Paolo Cuppini, Bologna, 1912.
#'
#'
#' @param x numeric vector of non-negative values
#'
#' @return The value of the inequality index, a number in \eqn{[0, 1]}.
#'
#' @examples
#' gini_index(c(2, 2, 2, 2, 2))   # no inequality
#' gini_index(c(0, 0, 10, 0, 0))  # one has it all
#' gini_index(c(7, 0, 3, 0, 0))   # give to the poor, take away from the rich
#' gini_index(c(6, 0, 3, 1, 0))   # (a.k.a. Pigou-Dalton principle)
#' bonferroni_index(c(2, 2, 2, 2, 2))
#' bonferroni_index(c(0, 0, 10, 0, 0))
#' bonferroni_index(c(7, 0, 3, 0, 0))
#' bonferroni_index(c(6, 0, 3, 1, 0))
#' devergottini_index(c(2, 2, 2, 2, 2))
#' devergottini_index(c(0, 0, 10, 0, 0))
#' devergottini_index(c(7, 0, 3, 0, 0))
#' devergottini_index(c(6, 0, 3, 1, 0))
#'
#' @name inequality
#' @rdname inequality
#' @export
gini_index <- function(x) {
    .Call(`_genieclust_gini_index`, x)
}

#' @rdname inequality
#' @export
bonferroni_index <- function(x) {
    .Call(`_genieclust_bonferroni_index`, x)
}

#' @rdname inequality
#' @export
devergottini_index <- function(x) {
    .Call(`_genieclust_devergottini_index`, x)
}

.oldmst.matrix <- function(X, distance = "euclidean", M = 1L, cast_float32 = FALSE, verbose = FALSE) {
    .Call(`_genieclust_dot_oldmst_matrix`, X, distance, M, cast_float32, verbose)
}

.oldmst.dist <- function(d, M = 1L, verbose = FALSE) {
    .Call(`_genieclust_dot_oldmst_dist`, d, M, verbose)
}

